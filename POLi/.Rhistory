model_data[[i]] <- list(
x = x_val[pos],
y = y_val[pos,],
sequences = sequences[pos],
input = input[pos,]
)
}
names(model_data) <- sets
model_data <- c(model_data,tokenizer_data)
return(model_data)
}
model_data <- make_model_data(data = data,
x = "text",
y = "numeric_label",
n_classes = 27,
sets = c("train","test","valid"),
probs = c(0.5,0.3,0.2),
sampling_strat = "up")
keras::save_text_tokenizer(model_data$tokenizer,"tokenizer")
saveRDS(label_mapping,"label_mapping.rds")
rm(list = ls())
label_mapping <- readRDS("www/label_mapping.rds")
tokenizer <- load_text_tokenizer("www/tokenizer")
text <- "Der Klimawandel stellt eine Bedrohung für Deutschland dar. Mehr Investition in Schulen ist essentiell."
as.data.frame(unlist(strsplit(text, "\\.")))
a <- as.data.frame(unlist(strsplit(text, "\\.")))
clean_text(input = a, col = 1)
clean_text <- function(data, cols) {
for(i in 1:length(cols)) {
data[[cols[i]]] <- data[[cols[i]]] %>%
tolower() %>%
stringr::str_replace_all("[^[:alnum:] ]+", " ") %>%
stringr::str_replace_all("[[:digit:]]+", " ") %>%
trimws(which = "both") %>%
stringr::str_replace_all("[\\s]+", " ") %>%
strsplit(.," ") %>%
{
stopwords <- stopwords::stopwords("de", source = "snowball")
lapply(., function(i) paste(SnowballC::wordStem(i[!i %in% stopwords],"german"), collapse = " ")) %>%
unlist()
}
}
return(data)
}
clean_text(input = a, col = 1)
clean_text(data = a, col = 1)
clean_text(data = a, col = 1)[[1]]
text <- as.data.frame(unlist(strsplit(analysis_input, "\\.")))
text <- as.data.frame(unlist(strsplit(text, "\\.")))
text <- clean_text(data = text, col = 1)[[1]]
sequences <- keras::texts_to_sequences(tokenizer, text)
# text data + ideology codes
data <- list.files(path = "~/POLi/ideology_analysis_data/germany",
pattern = "\\.csv$",
full.names = TRUE) %>%
purrr::map_dfr(data.table::fread, header = TRUE, fill = TRUE) %>%
dplyr::select(text,cmp_code) %>%
dplyr::rename(code = cmp_code) %>%
tidyr::drop_na()
# codes + labels
codebook <- readxl::read_xlsx("~/POLi/codebook/codebook.xlsx") %>%
dplyr::select(2:6) %>%
dplyr::mutate(title = gsub(" ","-",gsub(":","",title)))
data <- dplyr::left_join(data, codebook, by = "code")
# clean text -------------------------------------------------------------------
# remove non alpha-numeric symbols + digits
# remove excess white space + stopwords
# stem tokens
# this improves prediction accuracy
clean_text <- function(data, cols) {
for(i in 1:length(cols)) {
data[[cols[i]]] <- data[[cols[i]]] %>%
tolower() %>%
stringr::str_replace_all("[^[:alnum:] ]+", " ") %>%
stringr::str_replace_all("[[:digit:]]+", " ") %>%
trimws(which = "both") %>%
stringr::str_replace_all("[\\s]+", " ") %>%
strsplit(.," ") %>%
{
stopwords <- stopwords::stopwords("de", source = "snowball")
lapply(., function(i) paste(SnowballC::wordStem(i[!i %in% stopwords],"german"), collapse = " ")) %>%
unlist()
}
}
return(data)
}
data <- clean_text(data,1) %>%
dplyr::mutate(sentence_id = 1:dplyr::n()) %>%
dplyr::select(sentence_id,text,title_aggregated) %>%
magrittr::set_colnames(c("sentence_id","text","label"))
rm(codebook)
## prep model data -------------------------------------------------------------
#change encoding
data$text <- iconv(data$text, "UTF-8", "latin1")
#drop sentences with fewer than 3 words >> necessary for creating skipgrams with window = 3
drop <- strsplit(data$text," ") %>%
lapply(length) %>%
unlist() %>%
{. < 3}
data <- data[!drop,]
# create label mapping >> allows us to join text labels to predictions later
data <- data %>%
dplyr::mutate(numeric_label = as.numeric(as.factor(label)) - 1)
label_mapping <- data %>%
dplyr::select(numeric_label, label) %>%
dplyr::distinct(numeric_label, .keep_all = TRUE) %>%
dplyr::arrange(numeric_label)
# create model data
make_model_data <- function(data, x, y, n_classes, sets, probs, sampling_strat = NULL) {
if(!all(sets %in% c("train","test","valid"))) {
stop("sets must contain only 'train','test' and 'valid'")
}
if(length(sets) != length(probs)) {
stop("must specify a sampling probability for each set")
}
if(!is.null(sampling_strat)) {
if(!sampling_strat %in% c("up","down")) {
stop("sampling_strat must be one of 'up' or 'down'")
}
}
data <- as.data.frame(data)
set <- sample(sets,nrow(data), prob = probs,replace = TRUE)
#prep x
x_val <- data[,x]
num_words <- length(unique(unlist(strsplit(x_val," "))))
#prep y
y_val <- keras::to_categorical(as.matrix(data[,y]), num_classes = n_classes)
#prep sequences
tokenizer <- keras::text_tokenizer(num_words = num_words) %>% keras::fit_text_tokenizer(x_val)
sequences <- keras::texts_to_sequences(tokenizer, x_val)
max_length <- max(sapply(sequences, length))
input <- keras::pad_sequences(sequences, maxlen = max_length)
tokenizer_data <- list(tokenizer = tokenizer, max_length = max_length, num_words = num_words)
model_data <- vector(mode = "list", length = length(sets))
# make data for each set
for(i in 1:length(sets)) {
pos <- (set == sets[i])
# apply imbalance fix to training data
if(sets[i] == "train" & !is.null(sampling_strat)) {
d <- data.frame(
pos = which(pos),
y = data[pos,y]
)
if(sampling_strat == "up") {
d <- caret::upSample(x = d[,-ncol(d)],
y = factor(d$y))
} else {
d <- caret::downSample(x = d[,-ncol(d)],
y = factor(d$y))
}
pos <- d$x
}
model_data[[i]] <- list(
x = x_val[pos],
y = y_val[pos,],
sequences = sequences[pos],
input = input[pos,]
)
}
names(model_data) <- sets
model_data <- c(model_data,tokenizer_data)
return(model_data)
}
model_data <- make_model_data(data = data,
x = "text",
y = "numeric_label",
n_classes = 27,
sets = c("train","test","valid"),
probs = c(0.5,0.3,0.2),
sampling_strat = "up")
model_data$max_length
text
sequences <- keras::texts_to_sequences(tokenizer, text)
input_text <- keras::pad_sequences(sequences, maxlen = 45)
input_text
input_text <- asplit(input_text,1)
input_text
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/")
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")
library(tfdeploy)
library(tfdeploy)
unload(tfdeploy)
unloadNamespace(tfdeploy)
library(tfdeploy)
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")
serve_savedmodel('www/saved_model/poli_model', browse = TRUE)
tddeploy::serve_savedmodel('www/saved_model/poli_model', browse = TRUE)
tfdeploy::serve_savedmodel('www/saved_model/poli_model', browse = TRUE)
library(tfdeploy)
library(dplyr)
library(stringr)
library(magrittr)
library(plotly)
library(shiny)
library(tfdeploy)
library(keras)
library(shinyjs)
library(stopwords)
library(SnowballC)
label_mapping <- readRDS("www/label_mapping.rds")
tokenizer <- load_text_tokenizer("www/tokenizer")
clean_text <- function(data, cols) {
for(i in 1:length(cols)) {
data[[cols[i]]] <- data[[cols[i]]] %>%
tolower() %>%
stringr::str_replace_all("[^[:alnum:] ]+", " ") %>%
stringr::str_replace_all("[[:digit:]]+", " ") %>%
trimws(which = "both") %>%
stringr::str_replace_all("[\\s]+", " ") %>%
strsplit(.," ") %>%
{
stopwords <- stopwords::stopwords("de", source = "snowball")
lapply(., function(i) paste(SnowballC::wordStem(i[!i %in% stopwords],"german"), collapse = " ")) %>%
unlist()
}
}
return(data)
}
analysis_input <- "Der Klimawandel stellt eine Bedrohung für Deutschland dar. Wir fordern mehr Investition in Bildung."
text <- as.data.frame(unlist(strsplit(analysis_input, "\\.")))
text <- clean_text(data = text, col = 1)[[1]]
sequences <- keras::texts_to_sequences(tokenizer, text)
input_text <- keras::pad_sequences(sequences, maxlen = 45)
input_text <- asplit(input_text,1)
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")
View(results)
results <- transform(list = results[[1]])
View(results)
transform <- function(list){
loop <- function(x){
el_i <- as.data.frame(t(list[[x]][[1]]))
return(el_i)
}
out <- purrr::map_dfr(1:length(list), ~loop(.x))
}
results <- transform(list = results[[1]])
View(results)
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")
results
results$predictions
do.coll(rbind,results$predictions)
do.call(rbind,results$predictions)
a <- do.call(rbind,results$predictions)
View(a)
rm(a)
as.data.frame(do.call(rbind,results$predictions))
a <- as.data.frame(do.call(rbind,results$predictions))
View(a)
rm(a)
lapply(results$predictions, as.vector)
lapply(results$predictions, as.vector) %>% do.call(rbind,.)
lapply(results$predictions, function(i) t(as.vector(i))) %>% do.call(rbind,.)
lapply(results$predictions, function(i) t(as.vector(i[[1]]))) %>% do.call(rbind,.)
a <- lapply(results$predictions, function(i) t(as.vector(i[[1]]))) %>% do.call(rbind,.)
View(a)
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")$predictions %>%
lapply(., function(i) t(as.vector(i[[1]]))) %>%
do.call(rbind, .) %>%
as.data.frame()
results
rm(a)
View(results)
colnames(results) <- as.character(label_mapping$label)
results <- results %>%
dplyr::summarise_all(., mean)
View(results)
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")$predictions %>%
lapply(., function(i) t(as.vector(i[[1]]))) %>%
do.call(rbind, .) %>%
as.data.frame()
colnames(results) <- as.character(label_mapping$label)
r <- results %>%
dplyr::summarise_all(., mean)
r
r[,1]
results[,1]
mean(results[,1])
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")$predictions %>%
lapply(., function(i) t(as.vector(i[[1]]))) %>%
do.call(rbind, .) %>%
as.data.frame()
colnames(results) <- as.character(label_mapping$label)
results <- results %>%
dplyr::summarise_all(., mean) %>%
t()
View(results)
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")$predictions %>%
lapply(., function(i) t(as.vector(i[[1]]))) %>%
do.call(rbind, .) %>%
as.data.frame()
colnames(results) <- as.character(label_mapping$label)
results <- results %>%
dplyr::summarise_all(., mean) %>%
tidyr::pivot_longer(c(1:27))
View(results)
View(label_mapping)
View(results)
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")$predictions %>%
lapply(., function(i) t(as.vector(i[[1]]))) %>%
do.call(rbind, .) %>%
as.data.frame()
colnames(results) <- as.character(label_mapping$label)
results <- results %>%
dplyr::summarise_all(., mean) %>%
tidyr::pivot_longer(c(1:27)) %>%
dplyr::rename(label = name,
probability = value) %>%
dplyr::left_join(., label_mapping, by = "label")
View(results)
View(label_mapping)
View(results)
# codes + labels
codebook <- readxl::read_xlsx("~/POLi/codebook/codebook.xlsx") %>%
dplyr::select(2:6) %>%
dplyr::mutate(title = gsub(" ","-",gsub(":","",title)))
View(codebook)
View(label_mapping)
View(codebook)
a <- readRDS("~/label_mapping_old.rds")
View(a)
label_mapping <- readRDS("www/label_mapping.rds")
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")$predictions %>%
lapply(., function(i) t(as.vector(i[[1]]))) %>%
do.call(rbind, .) %>%
as.data.frame()
colnames(results) <- as.character(label_mapping$label)
results <- results %>%
dplyr::summarise_all(., mean) %>%
tidyr::pivot_longer(c(1:27)) %>%
dplyr::rename(label = name,
probability = value) %>%
dplyr::left_join(., label_mapping, by = "label")
View(results)
View(results)
View(results)
plot <- ggplot(data = results, aes(x = reorder(label, probability), y = probability, label = description))+
geom_bar(stat = "identity")+
scale_y_continuous(limits = c(0,1), expand = c(0,0))+
ylab("Classification Probability")+
coord_flip()+
theme_bw()+
theme(axis.title.y = element_blank())
plot
text
analysis_input <- "Es ist eine Kernaufgabe staatlichen Handelns, Rahmenbedingungen für gesunde, sichere und menschengerecht gestaltete Arbeitsbedingungen der Beschäftigten zu schaffen."
text <- as.data.frame(unlist(strsplit(analysis_input, "\\.")))
text <- clean_text(data = text, col = 1)[[1]]
sequences <- keras::texts_to_sequences(tokenizer, text)
input_text <- keras::pad_sequences(sequences, maxlen = 45)
input_text <- asplit(input_text,1)
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")$predictions %>%
lapply(., function(i) t(as.vector(i[[1]]))) %>%
do.call(rbind, .) %>%
as.data.frame()
colnames(results) <- as.character(label_mapping$label)
results <- results %>%
dplyr::summarise_all(., mean) %>%
tidyr::pivot_longer(c(1:27)) %>%
dplyr::rename(label = name,
probability = value) %>%
dplyr::left_join(., label_mapping, by = "label")
plot <- ggplot(data = results, aes(x = reorder(label, probability), y = probability, label = description))+
geom_bar(stat = "identity")+
scale_y_continuous(limits = c(0,1), expand = c(0,0))+
ylab("Classification Probability")+
coord_flip()+
theme_bw()+
theme(axis.title.y = element_blank())
plot
text
ui <- navbarPage(
"POLi (beta 0.1.0)",
tabPanel("Analysis",
useShinyjs(),
div(
textAreaInput("text_input",label = NULL, width = "4000px", height = "100px", placeholder = "enter your text...",
resize = "none")
),
actionButton("run","Analyze"),
actionButton("clear","Clear"),
plotlyOutput("plotly_out", width = "75%")
),
tabPanel("About",
"POLi is a natural language processing model designed to predict the ideological position
expressed in german political text. It utilizes custom word embeddings fed into a CNN LSTM
neural network architecture to generate ideology predictions. POLi was trained on ideology
labeled german party manifesto text data from The Manifesto Project [1]. As the data set used to
train POLi is comparitively small (by machine learning standards) and domain specific, the model
will likely not generalize well beyond the immediate context of german political texts (i.e. parliamentary bills,
policy proposals, parliamentary inquiries etc.). Within this domain; however, initial tests indicate promising performance.",
tags$footer("[1] Volkens, Andrea / Burst, Tobias / Krause, Werner / Lehmann, Pola / Matthiess Theres / Merz, Nicolas / Regel, Sven / Wessels, Bernhard / Zehnter, Lisa (2020): The Manifesto Data Collection. Manifesto Project (MRG/CMP/MARPOR). Version 2020b. Berlin: Wissenschaftszentrum Berlin fuer Sozialforschung (WZB).",
align = "left", style = "
position:absolute;
bottom:0;
width:100%;
height:50px;
color: black;
padding: 10px;
z-index: 1000;")
)
)
server <- function(input, output, session) {
values <- reactive(input$text_input)
plot <- eventReactive(input$run,{
progress <- shiny::Progress$new()
progress$set(message = "running model", value = 0)
on.exit(progress$close())
updateProgress <- function(value = NULL, detail = NULL) {
if (is.null(value)) {
value <- progress$getValue()
value <- value + (progress$getMax() - value) / 2
}
progress$set(value = value, detail = detail)
}
analysis(analysis_input = values(), prog = updateProgress)
})
output$plotly_out <- renderPlotly(
plotly_plot <- ggplotly(plot(), tooltip = c("probability","description"))%>%
style(hoverlabel = list(align = "left"))
)
observeEvent(input$clear,{
reset("text_input")
})
session$allowReconnect(FALSE)
}
shinyApp(ui, server)
analysis <- function(analysis_input, prog = NULL){
if (is.function(prog)) {
text <- "preparing model input"
prog(detail = text)
}
text <- as.data.frame(unlist(strsplit(analysis_input, "\\.")))
text <- clean_text(data = text, col = 1)[[1]]
sequences <- keras::texts_to_sequences(tokenizer, text)
input_text <- keras::pad_sequences(sequences, maxlen = 45)
input_text <- asplit(input_text,1)
if (is.function(prog)) {
text <- "computing predictions"
prog(detail = text)
}
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")$predictions %>%
lapply(., function(i) t(as.vector(i[[1]]))) %>%
do.call(rbind, .) %>%
as.data.frame()
colnames(results) <- as.character(label_mapping$label)
results <- results %>%
dplyr::summarise_all(., mean) %>%
tidyr::pivot_longer(c(1:27)) %>%
dplyr::rename(label = name,
probability = value) %>%
dplyr::left_join(., label_mapping, by = "label")
plot <- ggplot(data = results, aes(x = reorder(label, probability), y = probability, label = description))+
geom_bar(stat = "identity")+
scale_y_continuous(limits = c(0,1), expand = c(0,0))+
ylab("Classification Probability")+
coord_flip()+
theme_bw()+
theme(axis.title.y = element_blank())
return(plot)
}
shinyApp(ui, server)
analysis <- function(analysis_input, prog = NULL){
if (is.function(prog)) {
text <- "preparing model input"
prog(detail = text)
}
text <- as.data.frame(unlist(strsplit(analysis_input, "\\.")))
text <- clean_text(data = text, col = 1)[[1]]
sequences <- keras::texts_to_sequences(tokenizer, text)
input_text <- keras::pad_sequences(sequences, maxlen = 45)
input_text <- asplit(input_text,1)
if (is.function(prog)) {
text <- "computing predictions"
prog(detail = text)
}
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")$predictions %>%
lapply(., function(i) t(as.vector(i[[1]]))) %>%
do.call(rbind, .) %>%
as.data.frame()
colnames(results) <- as.character(label_mapping$label)
results <- results %>%
dplyr::summarise_all(., mean) %>%
tidyr::pivot_longer(c(1:27)) %>%
dplyr::rename(label = name,
probability = value) %>%
dplyr::left_join(., label_mapping, by = "label") %>%
dplyr::mutate(name = gsub("_", " ", label))
plot <- ggplot(data = results, aes(x = reorder(name, probability), y = probability, label = description))+
geom_bar(stat = "identity")+
scale_y_continuous(limits = c(0,1), expand = c(0,0))+
ylab("Classification Probability")+
coord_flip()+
theme_bw()+
theme(axis.title.y = element_blank())
return(plot)
}
shinyApp(ui, server)
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
