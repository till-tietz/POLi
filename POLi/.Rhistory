shiny::runApp()
rsconnect::deployApp(appDir="C:/Users/ttiet/Desktop/pol_app/POLi",logLevel="verbose",lint=TRUE)
rsconnect::deployApp(appDir="C:/Users/ttiet/Desktop/pol_app/POLi",logLevel="verbose",lint=TRUE)
rsconnect::deployApp(appDir="C:/Users/ttiet/Desktop/pol_app/POLi",logLevel="verbose",lint=TRUE)
options(rsconnect.max.bundle.size=1000000000)
rsconnect::deployApp(appDir="C:/Users/ttiet/Desktop/pol_app/POLi",logLevel="verbose",lint=TRUE)
rsconnect::showLogs()
runApp()
shiny::runApp()
?rsconnect::rsconnectOptions
file.edit(".Rprofile")
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
tensorflow::tf$Session()
runApp()
runApp()
runApp()
k_get_session()
tf$compat$v1$keras$backend$get_session()
sess <- tf$Session()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
install.packages("Shinyjs")
install.packages("shinyjs")
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
runApp()
tfsess <- tf$compat$v1$keras$backend$get_session()
library(tidyverse)
library(plotly)
library(shiny)
library(tfdeploy)
library(keras)
library(tensorflow)
library(shinyjs)
tfsess <- tf$compat$v1$keras$backend$get_session()
tf$compat$v1$keras$backend$set_session(tfsess)
graph <- load_savedmodel(sess,"www/savedmodel")
graph <- load_savedmodel(tfsess,"www/savedmodel")
graph <- load_savedmodel(tfsess,"www/savedmodel")
gc()
tfsess <- tf$compat$v1$keras$backend$get_session()
tf$compat$v1$keras$backend$set_session(tfsess)
graph <- load_savedmodel(tfsess,"www/savedmodel")
runApp()
runApp()
k_get_session()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
library(tidyverse)
library(plotly)
library(shiny)
library(tfdeploy)
library(keras)
library(shinyjs)
k_set_session()
tfsess <-tf$compat$v1$keras$backend$set_session()
library(tidyverse)
library(plotly)
library(shiny)
library(tfdeploy)
library(keras)
library(shinyjs)
tfsess <-tf$compat$v1$keras$backend$set_session()
tfsess <- tf$compat$v1$keras$backend$set_session()
runApp()
runApp()
tfsess <- tf$compat$v1$keras$backend$set_session()
shiny::runApp()
runApp()
runApp()
runApp()
runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
label_mapping <- readRDS("www/label_mapping.rds")
View(label_mapping)
rm(list = ls())
if(!"pacman" %in% installed.packages()) {
install.packages("pacman")
}
pacman::p_load(dplyr,tidytext,stopwords,SnowballC,
text2vec,keras,data.table,magrittr)
# text data + ideology codes
data <- list.files(path = "~/POLi/ideology_analysis_data/germany",
pattern = "\\.csv$",
full.names = TRUE) %>%
purrr::map_dfr(data.table::fread, header = TRUE, fill = TRUE) %>%
dplyr::select(text,cmp_code) %>%
dplyr::rename(code = cmp_code) %>%
tidyr::drop_na()
# codes + labels
codebook <- readxl::read_xlsx("~/POLi/codebook/codebook.xlsx") %>%
dplyr::select(2:6) %>%
dplyr::mutate(title = gsub(" ","-",gsub(":","",title)))
data <- dplyr::left_join(data, codebook, by = "code")
# remove non alpha-numeric symbols + digits
# remove excess white space + stopwords
# stem tokens
# this improves prediction accuracy
clean_text <- function(data, cols) {
for(i in 1:length(cols)) {
data[[cols[i]]] <- data[[cols[i]]] %>%
tolower() %>%
stringr::str_replace_all("[^[:alnum:] ]+", " ") %>%
stringr::str_replace_all("[[:digit:]]+", " ") %>%
trimws(which = "both") %>%
stringr::str_replace_all("[\\s]+", " ") %>%
strsplit(.," ") %>%
{
stopwords <- stopwords::stopwords("de", source = "snowball")
lapply(., function(i) paste(SnowballC::wordStem(i[!i %in% stopwords],"german"), collapse = " ")) %>%
unlist()
}
}
return(data)
}
data <- clean_text(data,1) %>%
dplyr::mutate(sentence_id = 1:dplyr::n()) %>%
dplyr::select(sentence_id,text,title_aggregated) %>%
magrittr::set_colnames(c("sentence_id","text","label"))
rm(codebook)
## prep model data -------------------------------------------------------------
#change encoding
data$text <- iconv(data$text, "UTF-8", "latin1")
#drop sentences with fewer than 3 words >> necessary for creating skipgrams with window = 3
drop <- strsplit(data$text," ") %>%
lapply(length) %>%
unlist() %>%
{. < 3}
data <- data[!drop,]
# create label mapping >> allows us to join text labels to predictions later
data <- data %>%
dplyr::mutate(numeric_label = as.numeric(as.factor(label)) - 1)
label_mapping <- data %>%
dplyr::select(numeric_label, label) %>%
dplyr::distinct(numeric_label, .keep_all = TRUE) %>%
dplyr::arrange(numeric_label)
# create model data
make_model_data <- function(data, x, y, n_classes, sets, probs, sampling_strat = NULL) {
if(!all(sets %in% c("train","test","valid"))) {
stop("sets must contain only 'train','test' and 'valid'")
}
if(length(sets) != length(probs)) {
stop("must specify a sampling probability for each set")
}
if(!is.null(sampling_strat)) {
if(!sampling_strat %in% c("up","down")) {
stop("sampling_strat must be one of 'up' or 'down'")
}
}
data <- as.data.frame(data)
set <- sample(sets,nrow(data), prob = probs,replace = TRUE)
#prep x
x_val <- data[,x]
num_words <- length(unique(unlist(strsplit(x_val," "))))
#prep y
y_val <- keras::to_categorical(as.matrix(data[,y]), num_classes = n_classes)
#prep sequences
tokenizer <- keras::text_tokenizer(num_words = num_words) %>% keras::fit_text_tokenizer(x_val)
sequences <- keras::texts_to_sequences(tokenizer, x_val)
max_length <- max(sapply(sequences, length))
input <- keras::pad_sequences(sequences, maxlen = max_length)
tokenizer_data <- list(tokenizer = tokenizer, max_length = max_length, num_words = num_words)
model_data <- vector(mode = "list", length = length(sets))
# make data for each set
for(i in 1:length(sets)) {
pos <- (set == sets[i])
# apply imbalance fix to training data
if(sets[i] == "train" & !is.null(sampling_strat)) {
d <- data.frame(
pos = which(pos),
y = data[pos,y]
)
if(sampling_strat == "up") {
d <- caret::upSample(x = d[,-ncol(d)],
y = factor(d$y))
} else {
d <- caret::downSample(x = d[,-ncol(d)],
y = factor(d$y))
}
pos <- d$x
}
model_data[[i]] <- list(
x = x_val[pos],
y = y_val[pos,],
sequences = sequences[pos],
input = input[pos,]
)
}
names(model_data) <- sets
model_data <- c(model_data,tokenizer_data)
return(model_data)
}
model_data <- make_model_data(data = data,
x = "text",
y = "numeric_label",
n_classes = 27,
sets = c("train","test","valid"),
probs = c(0.5,0.3,0.2),
sampling_strat = "up")
keras::save_text_tokenizer(model_data$tokenizer,"tokenizer")
saveRDS(label_mapping,"label_mapping.rds")
rm(list = ls())
label_mapping <- readRDS("www/label_mapping.rds")
tokenizer <- load_text_tokenizer("www/tokenizer")
text <- "Der Klimawandel stellt eine Bedrohung fÃ¼r Deutschland dar. Mehr Investition in Schulen ist essentiell."
as.data.frame(unlist(strsplit(text, "\\.")))
a <- as.data.frame(unlist(strsplit(text, "\\.")))
clean_text(input = a, col = 1)
clean_text <- function(data, cols) {
for(i in 1:length(cols)) {
data[[cols[i]]] <- data[[cols[i]]] %>%
tolower() %>%
stringr::str_replace_all("[^[:alnum:] ]+", " ") %>%
stringr::str_replace_all("[[:digit:]]+", " ") %>%
trimws(which = "both") %>%
stringr::str_replace_all("[\\s]+", " ") %>%
strsplit(.," ") %>%
{
stopwords <- stopwords::stopwords("de", source = "snowball")
lapply(., function(i) paste(SnowballC::wordStem(i[!i %in% stopwords],"german"), collapse = " ")) %>%
unlist()
}
}
return(data)
}
clean_text(input = a, col = 1)
clean_text(data = a, col = 1)
clean_text(data = a, col = 1)[[1]]
text <- as.data.frame(unlist(strsplit(analysis_input, "\\.")))
text <- as.data.frame(unlist(strsplit(text, "\\.")))
text <- clean_text(data = text, col = 1)[[1]]
sequences <- keras::texts_to_sequences(tokenizer, text)
# text data + ideology codes
data <- list.files(path = "~/POLi/ideology_analysis_data/germany",
pattern = "\\.csv$",
full.names = TRUE) %>%
purrr::map_dfr(data.table::fread, header = TRUE, fill = TRUE) %>%
dplyr::select(text,cmp_code) %>%
dplyr::rename(code = cmp_code) %>%
tidyr::drop_na()
# codes + labels
codebook <- readxl::read_xlsx("~/POLi/codebook/codebook.xlsx") %>%
dplyr::select(2:6) %>%
dplyr::mutate(title = gsub(" ","-",gsub(":","",title)))
data <- dplyr::left_join(data, codebook, by = "code")
# clean text -------------------------------------------------------------------
# remove non alpha-numeric symbols + digits
# remove excess white space + stopwords
# stem tokens
# this improves prediction accuracy
clean_text <- function(data, cols) {
for(i in 1:length(cols)) {
data[[cols[i]]] <- data[[cols[i]]] %>%
tolower() %>%
stringr::str_replace_all("[^[:alnum:] ]+", " ") %>%
stringr::str_replace_all("[[:digit:]]+", " ") %>%
trimws(which = "both") %>%
stringr::str_replace_all("[\\s]+", " ") %>%
strsplit(.," ") %>%
{
stopwords <- stopwords::stopwords("de", source = "snowball")
lapply(., function(i) paste(SnowballC::wordStem(i[!i %in% stopwords],"german"), collapse = " ")) %>%
unlist()
}
}
return(data)
}
data <- clean_text(data,1) %>%
dplyr::mutate(sentence_id = 1:dplyr::n()) %>%
dplyr::select(sentence_id,text,title_aggregated) %>%
magrittr::set_colnames(c("sentence_id","text","label"))
rm(codebook)
## prep model data -------------------------------------------------------------
#change encoding
data$text <- iconv(data$text, "UTF-8", "latin1")
#drop sentences with fewer than 3 words >> necessary for creating skipgrams with window = 3
drop <- strsplit(data$text," ") %>%
lapply(length) %>%
unlist() %>%
{. < 3}
data <- data[!drop,]
# create label mapping >> allows us to join text labels to predictions later
data <- data %>%
dplyr::mutate(numeric_label = as.numeric(as.factor(label)) - 1)
label_mapping <- data %>%
dplyr::select(numeric_label, label) %>%
dplyr::distinct(numeric_label, .keep_all = TRUE) %>%
dplyr::arrange(numeric_label)
# create model data
make_model_data <- function(data, x, y, n_classes, sets, probs, sampling_strat = NULL) {
if(!all(sets %in% c("train","test","valid"))) {
stop("sets must contain only 'train','test' and 'valid'")
}
if(length(sets) != length(probs)) {
stop("must specify a sampling probability for each set")
}
if(!is.null(sampling_strat)) {
if(!sampling_strat %in% c("up","down")) {
stop("sampling_strat must be one of 'up' or 'down'")
}
}
data <- as.data.frame(data)
set <- sample(sets,nrow(data), prob = probs,replace = TRUE)
#prep x
x_val <- data[,x]
num_words <- length(unique(unlist(strsplit(x_val," "))))
#prep y
y_val <- keras::to_categorical(as.matrix(data[,y]), num_classes = n_classes)
#prep sequences
tokenizer <- keras::text_tokenizer(num_words = num_words) %>% keras::fit_text_tokenizer(x_val)
sequences <- keras::texts_to_sequences(tokenizer, x_val)
max_length <- max(sapply(sequences, length))
input <- keras::pad_sequences(sequences, maxlen = max_length)
tokenizer_data <- list(tokenizer = tokenizer, max_length = max_length, num_words = num_words)
model_data <- vector(mode = "list", length = length(sets))
# make data for each set
for(i in 1:length(sets)) {
pos <- (set == sets[i])
# apply imbalance fix to training data
if(sets[i] == "train" & !is.null(sampling_strat)) {
d <- data.frame(
pos = which(pos),
y = data[pos,y]
)
if(sampling_strat == "up") {
d <- caret::upSample(x = d[,-ncol(d)],
y = factor(d$y))
} else {
d <- caret::downSample(x = d[,-ncol(d)],
y = factor(d$y))
}
pos <- d$x
}
model_data[[i]] <- list(
x = x_val[pos],
y = y_val[pos,],
sequences = sequences[pos],
input = input[pos,]
)
}
names(model_data) <- sets
model_data <- c(model_data,tokenizer_data)
return(model_data)
}
model_data <- make_model_data(data = data,
x = "text",
y = "numeric_label",
n_classes = 27,
sets = c("train","test","valid"),
probs = c(0.5,0.3,0.2),
sampling_strat = "up")
model_data$max_length
text
sequences <- keras::texts_to_sequences(tokenizer, text)
input_text <- keras::pad_sequences(sequences, maxlen = 45)
input_text
input_text <- asplit(input_text,1)
input_text
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/")
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")
library(tfdeploy)
library(tfdeploy)
unload(tfdeploy)
unloadNamespace(tfdeploy)
library(tfdeploy)
results <- tfdeploy::predict_savedmodel(input_text, "www/saved_model/poli_model")
serve_savedmodel('www/saved_model/poli_model', browse = TRUE)
tddeploy::serve_savedmodel('www/saved_model/poli_model', browse = TRUE)
tfdeploy::serve_savedmodel('www/saved_model/poli_model', browse = TRUE)
library(tfdeploy)
